{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328d9b4f",
   "metadata": {},
   "source": [
    "# Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adf78d",
   "metadata": {},
   "source": [
    "**Inspecting web pages with HTML**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035f527",
   "metadata": {},
   "source": [
    "If we go to any page in the google and right click to select 'inspect' and it will lead to the HTML script which the web page is made of and if we click to the arrow in a box on the left side of that tool bar it helps to select individual part of the HTML page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37736e41",
   "metadata": {},
   "source": [
    "**Beautiful Soup and Requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e68c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f88a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.scrapethissite.com/pages/forms/'\n",
    "# requests.get(url)\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd6898",
   "metadata": {},
   "source": [
    "If we get 204 instead of <Response [200]> it means that there is content in the actual webpage, 400 means a bad request, 401, 404 means the server cannot be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42740d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup(page.text, 'html') [parsing the page in html format]\n",
    "soup = BeautifulSoup(page.text, 'html')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()) # Making it visually beautiful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c07fb0",
   "metadata": {},
   "source": [
    "**Find and Find_All**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718397a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.scrapethissite.com/pages/forms/'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div') # It will show the very first div tag details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', class_ = \"col-md-12\") # Class is used to individualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_ = \"lead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98125b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p', class_ = \"lead\").text # .text doesn't work with find_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70092821",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p', class_ = \"lead\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0460bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fece01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('th').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf2d1c",
   "metadata": {},
   "source": [
    "# Portfolio - Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find('table', class_ =\"wikitable sortable\")\n",
    "table = soup.find_all('table')[1]\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all('th')\n",
    "table_titles = table.find_all('th') # instead of soup you have to look for the table\n",
    "table_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033df45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_titles = [titles.text.strip() for titles in table_titles]\n",
    "print(loop_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data = table.find_all('tr')\n",
    "print(column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(columns = loop_titles)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in column_data[1:]:  # Inorder to remove the empty list to get rid of error we define it as [1:]\n",
    "    row_data = row.find_all('td')\n",
    "    table_rows = [rows.text.strip() for rows in row_data]\n",
    "\n",
    "    length = len(df1)\n",
    "    df1.loc[length] = table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92eab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'Downloads\\Companies.csv', index = False) # save as CSV files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
